# .github/workflows/update_scrapers.yml

name: Actualización Diaria de Scrapers

on:
  # Permite ejecutar este workflow manualmente desde la pestaña "Actions"
  workflow_dispatch:

  # Configura la ejecución programada (cron job)
  schedule:
    # Se ejecuta a las 11:00 UTC, que son las 5:00 AM en Nicaragua (UTC-6)
    - cron: '0 11 * * *'

jobs:
  build-and-commit:
    runs-on: ubuntu-latest

    steps:
      # 1. Descarga el código de tu repositorio al entorno de ejecución
      - name: Checkout del repositorio
        uses: actions/checkout@v4

      # 2. Configura el entorno de Python
      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # Puedes usar la versión de Python que prefieras

      # 3. Instala las dependencias del proyecto
      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. Ejecuta el script principal que lanza todos los scrapers
      - name: Ejecutar los scrapers
        run: python run_all.py

      # 5. Configura Git y confirma los cambios si los hay
      - name: Commit y Push de los datos actualizados
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions-bot@github.com'
          # Añade todos los cambios (incluyendo los nuevos archivos JSON en data/)
          git add -A
          # Crea el commit solo si hay cambios que confirmar
          git diff --staged --quiet || git commit -m "Actualización automática de datos de scrapers"
          # Empuja los cambios a la rama principal
          git push